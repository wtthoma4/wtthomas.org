<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="27 Million Read Errors" /><meta property="og:locale" content="en" /><meta name="description" content="I recently responded to a data-threatening disk failure on my NAS, this article catalogues my response." /><meta property="og:description" content="I recently responded to a data-threatening disk failure on my NAS, this article catalogues my response." /><link rel="canonical" href="https://www.wtthomas.org/posts/27-Million-Read-Errors/" /><meta property="og:url" content="https://www.wtthomas.org/posts/27-Million-Read-Errors/" /><meta property="og:site_name" content="Taylor Thomas" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-06-19T04:00:00-04:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="27 Million Read Errors" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-21T04:00:00-04:00","datePublished":"2025-06-19T04:00:00-04:00","description":"I recently responded to a data-threatening disk failure on my NAS, this article catalogues my response.","headline":"27 Million Read Errors","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.wtthomas.org/posts/27-Million-Read-Errors/"},"url":"https://www.wtthomas.org/posts/27-Million-Read-Errors/"}</script><title>27 Million Read Errors | Taylor Thomas</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Taylor Thomas"><meta name="application-name" content="Taylor Thomas"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">Taylor Thomas</a><p class="site-subtitle fst-italic mb-0">CS @ NCSU. Focused on privacy, security, and steak.</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/cv/" class="nav-link"> <i class="fa-fw fas fa-file-pdf"></i> <span>CV</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/wtthoma4" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://www.wtthomas.org" aria-label="links" target="_blank" rel="noopener noreferrer" > <i class="fa-solid fa-link"></i> </a> <a href="javascript:location.href = 'mailto:' + ['taylor','wtthomas.org'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/taylor-t-a18294266/" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>27 Million Read Errors</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>27 Million Read Errors</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1750320000" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jun 19, 2025 </time> </span> <span> Updated <time data-ts="1750492800" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jun 21, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/wtthoma4">Taylor Thomas</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2932 words" > <em>16 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">27 Million Read Errors</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">27 Million Read Errors</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><p>I recently responded to a data-threatening disk failure on my NAS, this article catalogues my response.</p><blockquote class="prompt-danger"><p>I take no responsibility for data loss as a result of repeating the contents of this article. Your hardware and software configuration differ from mine. Always consult up-to-date documentation from hardware and software providers. Backups are king!</p></blockquote><h1 id="failure-notification">Failure Notification</h1><p>This may be the most important part of this article. If your system has no way to push information to you, or to a place where you see it, you will probably discover a failure too late. TrueNAS offers several methods to push system notifications, I use two: Telegram and Email. I will briefly cover establishing both.</p><h2 id="truenas-email"><span class="me-2">TrueNAS Email</span><a href="#truenas-email" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>For Email alerts to work, you must first configure a <code class="language-plaintext highlighter-rouge">Send method</code> in the System &gt; General Settings. TrueNAS offers SMTP or Gmail OAuth. Follow your email provider’s instructions to configure this, and send a test email to confirm it works.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/test_email.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/test_email.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>Once you have established successful email delivery, go to System &gt; Alert Settings. Click <code class="language-plaintext highlighter-rouge">Add</code> and set your desired receiving email address for alerts. As discussed later, I configure my email to use the <code class="language-plaintext highlighter-rouge">Notice</code> alert level.</p><h2 id="truenas-telegram"><span class="me-2">TrueNAS Telegram</span><a href="#truenas-telegram" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Telegram has a robust ecosystem for bots, but creating and configuring a bot can be obtuse. You will need a <code class="language-plaintext highlighter-rouge">token</code> and a <code class="language-plaintext highlighter-rouge">chat ID</code> .</p><p>To get a token, you must create a bot. Telegram has an automated tool for this called BotFather. DM BotFather and use the <code class="language-plaintext highlighter-rouge">/start</code> command to get started. After you follow the prompts, he will spit out a token. Keep this token secure.</p><p>To get a Chat ID, you first need to decide who the bot should talk to. If you only want your notifications to come to your Telegram account, you can create a DM chat with the bot and supply that Chat ID. If you want multiple people to receive notifications (or if you want all your bots to send to the same place, if you have more than one bot), you can create a group chat and use that instead.</p><p>To acquire a chat ID for a one-on-one DM, first send your bot a message. You can click on the bot’s username in the BotFather DM to start that conversation. Send a message like “hello”. Next, use Telegram’s API to get the chat ID. Open a web browser and navigate to <a href="https://api.telegram.org/bot(BOT_TOKEN_HERE)/getUpdates">https://api.telegram.org/bot(BOT_TOKEN_HERE)/getUpdates</a>. Replace <code class="language-plaintext highlighter-rouge">(BOT_TOKEN_HERE)</code> with your bot token, no parentheses. You will see an output that contains the chat ID, similar to below. If the <code class="language-plaintext highlighter-rouge">result</code> JSON object is empty, send the bot another message and try again.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/chat_id.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/chat_id.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>To acquire a chat ID for a group chat, you’ll use the same steps as above, but you must create the group chat <strong>and give the bot adequate permissions</strong> before sending a message to the group chat. To add members, click on the group chat and click the plus icon. Search for the bot’s username and add it. If you send a message and don’t get a useful API result, consider right-clicking on the bot’s name in the group chat members list and clicking <code class="language-plaintext highlighter-rouge">Promote to Admin</code>.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/group_chat.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/group_chat.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>Once again, we are searching for the chat ID object in the JSON response.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/group_id.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/group_id.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>Now that you have a Chat ID and Bot Token, we can set it up in TrueNAS. Go to System &gt; Alert Settings and click <code class="language-plaintext highlighter-rouge">Add</code> in the Alert Services section. Under <code class="language-plaintext highlighter-rouge">Type</code>, select <code class="language-plaintext highlighter-rouge">Telegram</code>. Fill in the rest of the information. I advise starting with the <code class="language-plaintext highlighter-rouge">Level</code> set to <code class="language-plaintext highlighter-rouge">Info</code>. This may result in more alerts than you want, but it gives you the opportunity to refine TrueNAS’s alert levels system-wide. I will not cover this here, but TrueNAS offers granular alert controls. I personally leave my Email option on <code class="language-plaintext highlighter-rouge">Notice</code> and my Telegram bot on <code class="language-plaintext highlighter-rouge">Info</code>, without changing the default attributions for events.</p><blockquote class="prompt-tip"><p>Configure the Alert Settings appropriately for your software, hardware, and risk tolerance.</p></blockquote><p>Once everything is configured, send a test alert to confirm your configuration works. If not, review the steps above and consult Telegram’s documentation.</p><h2 id="my-alert"><span class="me-2">My Alert</span><a href="#my-alert" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Below is the Email alert that got my attention.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/alert_email.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/alert_email.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>Uh oh. Not good. Even though the pool is still online (as opposed to “degraded” or even worse, “offline”), unrecoverable error sounds pretty nasty. An additional piece of context is necessary: these drives are super old. Old drives plus <em>any</em> stability warning is a recipe for immediate intervention. After handling the situation I put the trouble drive on my test bench and found that it has logged 55,000 power-on hours. That’s more than six years of continuous operation. It had a good run.</p><h1 id="investigation">Investigation</h1><p>Step one, log in to the TrueNAS dashboard and see what we can see.</p><p>I see a red X next to my pool’s ZFS Health, that’s not ideal.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/ZFS_fail.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/ZFS_fail.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>I need more information, time to head to the shell. System &gt; Shell.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre><td class="rouge-code"><pre><span class="c"># Let's do a broad system sweep for errors.</span>
<span class="nv">$ </span><span class="nb">sudo </span>dmesg | <span class="nb">grep</span> <span class="nt">-i</span> error
...
a handful of errors about my ARCHIVE_POOL
...
<span class="c"># Let's see if the system journal has anything to say.</span>
<span class="nv">$ </span><span class="nb">sudo </span>journalctl <span class="nt">-xe</span> | <span class="nb">grep</span> <span class="nt">-i</span> <span class="s1">'kernel'</span> | <span class="nb">grep</span> <span class="nt">-i</span> <span class="s1">'error'</span>
...
Jun 17 19:11:30 TrueNAS zed[3450564]: <span class="nv">eid</span><span class="o">=</span>156 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485998592 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691389
Jun 17 19:11:30 TrueNAS zed[3450569]: <span class="nv">eid</span><span class="o">=</span>158 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485924864 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691387
Jun 17 19:11:30 TrueNAS zed[3450568]: <span class="nv">eid</span><span class="o">=</span>157 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485961728 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691388
Jun 17 19:11:30 TrueNAS zed[3450573]: <span class="nv">eid</span><span class="o">=</span>159 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485888000 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691386
Jun 17 19:11:30 TrueNAS zed[3450575]: <span class="nv">eid</span><span class="o">=</span>160 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485814272 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691384
Jun 17 19:11:30 TrueNAS zed[3450580]: <span class="nv">eid</span><span class="o">=</span>161 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485851136 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691385
Jun 17 19:11:30 TrueNAS zed[3450582]: <span class="nv">eid</span><span class="o">=</span>162 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485777408 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691383
Jun 17 19:11:30 TrueNAS zed[3450585]: <span class="nv">eid</span><span class="o">=</span>163 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485740544 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691381
Jun 17 19:11:30 TrueNAS zed[3450588]: <span class="nv">eid</span><span class="o">=</span>164 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485703680 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691380
Jun 17 19:11:30 TrueNAS zed[3450591]: <span class="nv">eid</span><span class="o">=</span>165 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485629952 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691382
Jun 17 19:11:30 TrueNAS zed[3450594]: <span class="nv">eid</span><span class="o">=</span>166 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485666816 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691379
Jun 17 19:11:30 TrueNAS zed[3450598]: <span class="nv">eid</span><span class="o">=</span>167 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>36864 <span class="nv">offset</span><span class="o">=</span>96485593088 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691378
Jun 17 19:11:30 TrueNAS zed[3450601]: <span class="nv">eid</span><span class="o">=</span>168 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485560320 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691377
Jun 17 19:11:30 TrueNAS zed[3450603]: <span class="nv">eid</span><span class="o">=</span>169 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485523456 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691376
Jun 17 19:11:30 TrueNAS zed[3450606]: <span class="nv">eid</span><span class="o">=</span>170 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485486592 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691375
Jun 17 19:11:30 TrueNAS zed[3450609]: <span class="nv">eid</span><span class="o">=</span>171 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485412864 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691373
Jun 17 19:11:30 TrueNAS zed[3450612]: <span class="nv">eid</span><span class="o">=</span>172 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485449728 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691374
Jun 17 19:11:30 TrueNAS zed[3450615]: <span class="nv">eid</span><span class="o">=</span>173 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485339136 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691371
Jun 17 19:11:30 TrueNAS zed[3450617]: <span class="nv">eid</span><span class="o">=</span>174 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485376000 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691372
Jun 17 19:11:30 TrueNAS zed[3450619]: <span class="nv">eid</span><span class="o">=</span>175 <span class="nv">class</span><span class="o">=</span>checksum <span class="nv">pool</span><span class="o">=</span><span class="s1">'ARCHIVE_POOL'</span> <span class="nv">vdev</span><span class="o">=</span>568edbff-2041-46ab-9b2f-77c381bcca24 <span class="nv">size</span><span class="o">=</span>32768 <span class="nv">offset</span><span class="o">=</span>96485302272 <span class="nv">priority</span><span class="o">=</span>4 <span class="nv">err</span><span class="o">=</span>0 <span class="nv">flags</span><span class="o">=</span>0x1000b0 <span class="nv">bookmark</span><span class="o">=</span>152:3769:0:691370
Jun 17 19:11:30 TrueNAS kernel: ata8.00: failed to <span class="nb">read </span>SCR 1 <span class="o">(</span><span class="nv">Emask</span><span class="o">=</span>0x40<span class="o">)</span>
Jun 17 19:11:30 TrueNAS kernel: ata8.01: failed to <span class="nb">read </span>SCR 1 <span class="o">(</span><span class="nv">Emask</span><span class="o">=</span>0x40<span class="o">)</span>
Jun 17 19:11:30 TrueNAS kernel: ata8.02: failed to <span class="nb">read </span>SCR 1 <span class="o">(</span><span class="nv">Emask</span><span class="o">=</span>0x40<span class="o">)</span>
Jun 17 19:11:30 TrueNAS kernel: ata8.03: failed to <span class="nb">read </span>SCR 1 <span class="o">(</span><span class="nv">Emask</span><span class="o">=</span>0x40<span class="o">)</span>
Jun 17 19:11:30 TrueNAS kernel: ata8.04: failed to <span class="nb">read </span>SCR 1 <span class="o">(</span><span class="nv">Emask</span><span class="o">=</span>0x40<span class="o">)</span>
Jun 17 19:11:30 TrueNAS kernel: ata8.04: exception Emask 0x100 SAct 0x100000 SErr 0x0 action 0x6 frozen
Jun 17 19:11:30 TrueNAS kernel: ata8.04: failed <span class="nb">command</span>: READ FPDMA QUEUED
Jun 17 19:11:30 TrueNAS kernel: ata8.04: cmd 60/d0:a0:08:aa:3b/07:00:0b:00:00/40 tag 20 ncq dma 1024000 <span class="k">in
                                            </span>res 00/00:01:09:4f:c2/00:00:00:00:00/00 Emask 0x2 <span class="o">(</span>HSM violation<span class="o">)</span>
Jun 17 19:11:30 TrueNAS kernel: ata8.15: SATA <span class="nb">link </span>up 6.0 Gbps <span class="o">(</span>SStatus 133 SControl 300<span class="o">)</span>
Jun 17 19:11:31 TrueNAS kernel: ata8.00: SATA <span class="nb">link </span>up 6.0 Gbps <span class="o">(</span>SStatus 133 SControl 330<span class="o">)</span>
Jun 17 19:11:31 TrueNAS kernel: ata8.01: SATA <span class="nb">link </span>up 6.0 Gbps <span class="o">(</span>SStatus 133 SControl 330<span class="o">)</span>
Jun 17 19:11:32 TrueNAS kernel: ata8.02: SATA <span class="nb">link </span>up 6.0 Gbps <span class="o">(</span>SStatus 133 SControl 330<span class="o">)</span>
Jun 17 19:11:32 TrueNAS kernel: ata8.03: SATA <span class="nb">link </span>up 6.0 Gbps <span class="o">(</span>SStatus 133 SControl 330<span class="o">)</span>
Jun 17 19:11:32 TrueNAS kernel: ata8.04: hard resetting <span class="nb">link
</span>Jun 17 19:11:33 TrueNAS kernel: ata8.04: SATA <span class="nb">link </span>up 6.0 Gbps <span class="o">(</span>SStatus 133 SControl 330<span class="o">)</span>
Jun 17 19:11:33 TrueNAS kernel: ata8.00: configured <span class="k">for </span>UDMA/133
Jun 17 19:11:33 TrueNAS kernel: ata8.01: configured <span class="k">for </span>UDMA/133
...
</pre></table></code></div></div><p>Woah!</p><p>Time for some vocabulary. <code class="language-plaintext highlighter-rouge">TrueNAS Scale</code> is an operating system for home/enthusiast servers. <code class="language-plaintext highlighter-rouge">TrueNAS Scale</code> uses <code class="language-plaintext highlighter-rouge">ZFS</code> (Zettabyte File System) as the backbone and core file system for all its operations. <code class="language-plaintext highlighter-rouge">ZFS</code> combines physical drives into <code class="language-plaintext highlighter-rouge">Zpools</code> (ZFS Pools) with spectacular performance for Mirroring, Compression, Deduplication, RAID Configurations, Backups, Scrubs, and more. <code class="language-plaintext highlighter-rouge">Zpools</code> can host one to many <code class="language-plaintext highlighter-rouge">Datasets</code>, one to many <code class="language-plaintext highlighter-rouge">Zvols</code>, or a mixture of the two. A <code class="language-plaintext highlighter-rouge">Dataset</code> gives the advantage of dynamic size scaling, access control lists, and connections to share services like SMB, FTP, or NFS, and they can be nested within each other. <code class="language-plaintext highlighter-rouge">Zvols</code> work more like virtual block devices and would be better for VMs or network boot devices.</p><p>Given that background, the error messages being related to my <code class="language-plaintext highlighter-rouge">ARCHIVE_POOL</code> tells me that something is going wrong at the zpool level. This typically indicates a physical drive failure, but it could also be a software/RAID problem or the work of a threat actor. The SATA kernel errors reinforce my suspicion that it’s a drive failure, but let’s see what’s going on:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="nv">$sudo</span> zpool status <span class="nt">-v</span> ARCHIVE_POOL
...
raidz1-0                                  ONLINE       0     0     0
    ac7328bc-fa24-4d20-84ba-4ac4d9e092c4  ONLINE       0     0     0
    23960ef1-f52b-4b31-937c-7231970f6b1d  ONLINE       0     0     0
    b4d7c3aa-7fba-45d9-94e5-1a4708567f0c  ONLINE       0     0     0
    568edbff-2041-46ab-9b2f-77c381bcca24  ONLINE       0     0    20
errors: No known data errors
...
</pre></table></code></div></div><p>Looks like we have 20 checksum errors on <code class="language-plaintext highlighter-rouge">568edbff-2041-46ab-9b2f-77c381bcca24</code>. What is that?</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="c"># This command will tell me what physical drive is associated with the ID</span>
<span class="c"># given in the status output.</span>
<span class="nv">$sudo</span> <span class="nb">ls</span> <span class="nt">-la</span> /dev/disk/by-partuuid/ | <span class="nb">grep </span>568edbff-2041-46ab-9b2f-77c381bcca24
lrwxrwxrwx 1 root root  10 Jun 17 19:18 568edbff-2041-46ab-9b2f-77c381bcca24 -&gt; ../../sde1
</pre></table></code></div></div><p>This tells me that <code class="language-plaintext highlighter-rouge">sde1</code> is the culprit device. Recall the Linux enumerates SATA drives with sdX or sdXY. <code class="language-plaintext highlighter-rouge">sde1</code> points to the first partition on the fifth SATA drive provisioned by the host. The next step is to run a SMART test and see if that reveals anything helpful:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo </span>smartctl <span class="nt">-a</span> /dev/sde1
...
SMART Attributes Data Structure revision number: 10
Vendor Specific SMART Attributes with Thresholds:
ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE
  1 Raw_Read_Error_Rate     0x000f   074   064   006    Pre-fail  Always       -       27095952
  3 Spin_Up_Time            0x0003   097   097   000    Pre-fail  Always       -       0
  4 Start_Stop_Count        0x0032   096   096   020    Old_age   Always       -       4669
  5 Reallocated_Sector_Ct   0x0033   100   100   010    Pre-fail  Always       -       0
  7 Seek_Error_Rate         0x000f   087   060   045    Pre-fail  Always       -       531146731
...
</pre></table></code></div></div><p>Holy smokes! 27 million read errors! This thing needs to go <strong>yesterday</strong>!</p><h1 id="remediation">Remediation</h1><p>TrueNAS’ ZFS implementation supports a wide set of options for resilvering. For best performance, leave the failing drive in the system and add the recovery drive so both are available to ZFS simultaneously. If the drive has no redundancy or backup, this is not optional. The set of circumstances where it’s appropriate to have a single point of failure in data storage is <strong>extremely limited</strong>. <strong>Do not do this</strong>. If your redundancy is configured as a one-to-one mirror, the performance is limited to the read speed of an intact drive(s) and the write speed of the replacement drive, so removing the failing drive in this scenario does not affect the speed of the resilvering process. If your redundancy is configured in a RAID array, removing the failing drive can dramatically slow down the process, as the failed drive must be reconstructed by reading from all the intact drives and doing a bunch of math. If you leave the failing drive in the system, you can just read from the failing drive and correct errors after completing the faster data copy operation.</p><p>In my situation, I have a RAID array configured with four drives. I have hot-swapping enabled in my motherboard’s BIOS, so I simply plugged in a replacement drive and proceeded with the remediation. I will show you how to check for hot-swapping later.</p><blockquote class="prompt-tip"><p>It is extremely preferable to avoid powering off the host machine when a drive is failing. If SATA hot-swapping is supported by your motherboard, I highly encourage you to turn it on before you need it!</p></blockquote><p>I will need to use ZFS’s <code class="language-plaintext highlighter-rouge">replace</code> command, which will handle all the drive resilvering for me. Before we do this, we must <code class="language-plaintext highlighter-rouge">offline</code> the failing drive. Doing this can affect how your entire pool performs, as ZFS becomes very conservative when a drive is down. In an enterprise environment, it is completely appropriate to take this system out of production. For clarity, doing this does <strong>not</strong> power off the drive, it simply removes it from operation in ZFS and puts the pool in a degraded state.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo </span>zpool offline ARCHIVE_POOL 568edbff-2041-46ab-9b2f-77c381bcca24
<span class="c"># Now confirm that drive is offlined.</span>
<span class="nv">$ </span><span class="nb">sudo </span>zpool status ARCHIVE_POOL
...
raidz1-0                                  DEGRADED     0     0     0
    ac7328bc-fa24-4d20-84ba-4ac4d9e092c4  ONLINE       0     0     0
    23960ef1-f52b-4b31-937c-7231970f6b1d  ONLINE       0     0     0
    b4d7c3aa-7fba-45d9-94e5-1a4708567f0c  ONLINE       0     0     0
    568edbff-2041-46ab-9b2f-77c381bcca24  OFFLINE      0     0    20
...
</pre></table></code></div></div><p>Now we need to get our new drive installed. To check for hot-swapping capability before we try (and potentially crash the system if we <strong>don’t</strong> have it enabled), we can check for AHCI (Advanced Host Controller Interface) support:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo </span>dmesg | <span class="nb">grep</span> <span class="nt">-i</span> <span class="s2">"ahci"</span>
<span class="o">[</span>    1.781542] ahci 0000:02:00.1: version 3.0
<span class="o">[</span>    1.781690] ahci 0000:02:00.1: SSS flag <span class="nb">set</span>, parallel bus scan disabled
<span class="o">[</span>    1.781745] ahci 0000:02:00.1: AHCI 0001.0301 32 slots 6 ports 6 Gbps 0x3f impl SATA mode
<span class="o">[</span>    1.781748] ahci 0000:02:00.1: flags: 64bit ncq sntf stag pm led clo only pmp pio slum part sxs deso sadm sds apst 
<span class="o">[</span>    1.783219] scsi host0: ahci
<span class="o">[</span>    1.783350] scsi host1: ahci
<span class="o">[</span>    1.783458] scsi host2: ahci
<span class="o">[</span>    1.783572] scsi host3: ahci
<span class="o">[</span>    1.783685] scsi host4: ahci
<span class="o">[</span>    1.783796] scsi host5: ahci
<span class="o">[</span>    1.784005] ahci 0000:05:00.0: SSS flag <span class="nb">set</span>, parallel bus scan disabled
<span class="o">[</span>    1.784049] ahci 0000:05:00.0: AHCI 0001.0200 32 slots 2 ports 6 Gbps 0x3 impl SATA mode
<span class="o">[</span>    1.784053] ahci 0000:05:00.0: flags: 64bit ncq sntf stag led clo pmp pio slum part ccc sxs 
<span class="o">[</span>    1.784610] scsi host6: ahci
<span class="o">[</span>    1.784756] scsi host7: ahci
</pre></table></code></div></div><p>Bingo! Hot swapping confirmed on my SATA ports. Seeing AHCI enabled is typically enough, but specifically we’re looking for PMP (port multiplier) and NCQ (native command queuing) which are both present on all my interfaces.</p><p>Before we insert the drive, we need a list of current drives to compare with once we insert the replacement drive:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo ls</span> <span class="nt">-la</span> /dev/disk/by-id/
... 
&lt;list of drives&gt;
...
</pre></table></code></div></div><p>After inserting the drive, do it again, and identify the drive that wasn’t in the original set. Because it’s not initialized in ZFS or added to a pool yet, we would expect to see the raw serial instead of a ZFS ID. I have redacted the drive serial.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo ls</span> <span class="nt">-la</span> /dev/disk/by-id/
... 
lrwxrwxrwx 1 root root  10 Jun 17 20:18 ata-[REDACTED] -&gt; ../../sdh
lrwxrwxrwx 1 root root  10 Jun 17 20:18 wwn-[REDACTED] -&gt; ../../sdh
...
</pre></table></code></div></div><p>These identifiers are what we need to execute the <code class="language-plaintext highlighter-rouge">replace</code> command and resilver the drive. ZFS will automatically handle initializing the drive and importing it into the pool for us. All the data on the replacement drive will be destroyed. The syntax for the <code class="language-plaintext highlighter-rouge">replace</code> command is as follows: <code class="language-plaintext highlighter-rouge">$ sudo zpool replace &lt;pool name&gt; &lt;existing drive ZFS identifier&gt; &lt;new drive identifier&gt;</code>. ZFS has options for auto-detection, but I prefer to be explicit. You can also use the drive letter <code class="language-plaintext highlighter-rouge">/sdh</code>, but I prefer to be explicit.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="nv">$ </span><span class="nb">sudo </span>zpool replace ARCHIVE_POOL 568edbff-2041-46ab-9b2f-77c381bcca24 /dev/disk/by-id/ata-[REDACTED]
<span class="c"># Now confirm the resilveirng has begun.</span>
<span class="nv">$ </span><span class="nb">sudo </span>zpool status ARCHIVE_POOL
...
raidz1-0                                  DEGRADED     0     0     0
    ac7328bc-fa24-4d20-84ba-4ac4d9e092c4  ONLINE       0     0     0
    23960ef1-f52b-4b31-937c-7231970f6b1d  ONLINE       0     0     0
    b4d7c3aa-7fba-45d9-94e5-1a4708567f0c  ONLINE       0     0     0
    replacing-3                                 DEGRADED     0     0     0
        568edbff-2041-46ab-9b2f-77c381bcca24    OFFLINE      0     0    20
        ata-[REDACTED]                          ONLINE       0     0     0 <span class="o">(</span>resilvering<span class="o">)</span>
...
</pre></table></code></div></div><p>You should also see a little status badge in the top-right corner of the GUI (open in a new tab, you will lose your shell history if you click away!):</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/resilvering.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/resilvering.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>From this point forward, let ZFS do its work. After the resilvering is complete, the failing drive will be removed from the pool, having been replaced. This drive will show up in the GUI as an unused disk, and at this point you can safely remove it.</p><h2 id="prevention"><span class="me-2">Prevention</span><a href="#prevention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>I have changed TrueNAS’ default scrub schedule for every drive in the system. By default, my system was running a scrub once every 35 days. Intuitively this is a problem, if a drive develops an issue one day after a scrub, you could be accumulating 34 days of errors before you know. I have updated all my scrub tasks to run daily, ensuring I can respond quickly and minimize any data loss.</p><p><a href="/assets/img/posts/2025-06-19-27 Million Read Errors/scrub_tasks.png" class="popup img-link w-100 shimmer"><img src="/assets/img/posts/2025-06-19-27 Million Read Errors/scrub_tasks.png" alt="Alert" class="mx-auto" loading="lazy"></a></p><p>Also, don’t be lazy. Have an offsite, ideally offline, backup.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/self-hosting/">Self Hosting</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/truenas/" class="post-tag no-text-decoration" >TrueNAS</a> <a href="/tags/storage/" class="post-tag no-text-decoration" >Storage</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/Custom-Domain,-Bot-Protection,-and-Region-Locks-on-GitHub-Pages-with-Cloudflare/">Custom Domain, Bot Protection, and Region Locks on GitHub Pages with Cloudflare</a><li class="text-truncate lh-lg"> <a href="/posts/27-Million-Read-Errors/">27 Million Read Errors</a><li class="text-truncate lh-lg"> <a href="/posts/HackPackCTF-FileForest/">HackPackCTF 2025: FileForest</a><li class="text-truncate lh-lg"> <a href="/posts/HackPackCTF-WeAreGreenLLC/">HackPackCTF 2025: WeAreGreenLLC</a><li class="text-truncate lh-lg"> <a href="/posts/VPN-with-Custom-DNS-on-iOS-Using-Passepartout/">VPN with Custom DNS on iOS Using Passepartout</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/dns/">DNS</a> <a class="post-tag btn btn-outline-primary" href="/tags/hackpackctf/">HackPackCTF</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloudflare/">Cloudflare</a> <a class="post-tag btn btn-outline-primary" href="/tags/github/">GitHub</a> <a class="post-tag btn btn-outline-primary" href="/tags/ios/">iOS</a> <a class="post-tag btn btn-outline-primary" href="/tags/storage/">Storage</a> <a class="post-tag btn btn-outline-primary" href="/tags/truenas/">TrueNAS</a> <a class="post-tag btn btn-outline-primary" href="/tags/vpn/">VPN</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/HackPackCTF-FileForest/" class="btn btn-outline-primary" aria-label="Older" ><p>HackPackCTF 2025: FileForest</p></a> <a href="/posts/Custom-Domain,-Bot-Protection,-and-Region-Locks-on-GitHub-Pages-with-Cloudflare/" class="btn btn-outline-primary" aria-label="Newer" ><p>Custom Domain, Bot Protection, and Region Locks on GitHub Pages with Cloudflare</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/wtthoma4">Taylor Thomas</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Red and White Forever!</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/dns/">DNS</a> <a class="post-tag btn btn-outline-primary" href="/tags/hackpackctf/">HackPackCTF</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloudflare/">Cloudflare</a> <a class="post-tag btn btn-outline-primary" href="/tags/github/">GitHub</a> <a class="post-tag btn btn-outline-primary" href="/tags/ios/">iOS</a> <a class="post-tag btn btn-outline-primary" href="/tags/storage/">Storage</a> <a class="post-tag btn btn-outline-primary" href="/tags/truenas/">TrueNAS</a> <a class="post-tag btn btn-outline-primary" href="/tags/vpn/">VPN</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
